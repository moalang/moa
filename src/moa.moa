class Token {
  code   str
  file   str
  offset int
  lineno int
  column int
}

class Node {
  token Token
}

def generate nodes {
  def genjs node {
  }
  return nodes.map(genjs).join("\n")
}

def tokenize program file {
  let reg r/(r\/(?:[^\/\\]|\\.)*?\/[A-Za-z]*|[0-9]+|(?:\.\.\.)?[A-Za-z0-9_]+|[\.()[\]{}]|"(?:[^"\\]|\\.)*"|`(?:[^`\\]|\\.)*`|[+\-*\/%<>!=^|&]+|#[^\n]*| +|\n+)/g
  var offset 0
  var lineno 1
  var column 1
  var tokens []
  each code program.match(reg) {
    if code.starts("\n") {
      lineno += code.count("\n")
      column = 1
    } else if code.starts(" ") {
      if column == 1 {
        column += code.size
      }
    } else if code.starts("#") {
      # do nothing
    } else {
      tokens.push Token(code file offset lineno column)
    }
    offset += code.size
  }
  return tokens
}

test t {
  def eq expected program {
    t.eq tokenize(program).map(t => t.code).join(" ") expected
  }
  eq "0 a b1 ( ) [ ] { } \"A\" `B` + ** %=" "0 a b1 ()[]{} \"A\" `B` + ** %= #comment"
  eq `"a\\"\\nb"` `"a\\"\\nb"`
  eq "r/a/" "r/a/"
  eq "r/\\\\/" "r/\\\\/"
  eq "r/a/g" "r/a/g"
  eq "r/a/ig" "r/a/ig"
}

def parse tokens {
  return []
}

def infer nodes {
  return nodes
}

test t {
}
